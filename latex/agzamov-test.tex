\documentclass[11pt,a4paper]{article}

% === Packages ===
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}
\usepackage{caption}
\usepackage{authblk}
\usepackage{natbib}
\usepackage{microtype}

\hypersetup{
  colorlinks=true,
  linkcolor=blue!70!black,
  citecolor=blue!70!black,
  urlcolor=blue!70!black,
}

\lstset{
  basicstyle=\small\ttfamily,
  frame=single,
  backgroundcolor=\color{gray!5},
  breaklines=true,
  columns=fullflexible,
  language=Python,
  keywordstyle=\color{blue!70!black},
  commentstyle=\color{green!50!black},
}

% === Title ===
\title{\textbf{The Agzamov Test:} \\ A Game-Theoretic Benchmark for Measuring AI Memory Infrastructure Quality Under Adversarial Conditions}

\author[1]{Ali Agzamov}
\affil[1]{BrainOps Limited, Queenstown, New Zealand}
\date{February 2026}

\begin{document}
\maketitle

% ============================================================
\begin{abstract}
Current AI benchmarks evaluate models in static, exam-like conditions---a question is asked, an answer is given, a score is assigned. This tells us nothing about performance in dynamic environments where adversaries adapt and historical context matters.

We introduce the \emph{Agzamov Test}, a dual-axis adversarial benchmark that uses repeated game-theoretic competition to measure both a model's ability to use memory effectively and the quality of the memory infrastructure itself. The test employs Chess960 (eliminating opening-book knowledge) and No-Limit Texas Hold'em poker across five phases: baseline, asymmetric memory, arms race with strategy-shift recovery, full orchestration with sub-phase decomposition, and positional stress testing. Synthetic opponent patterns---behavioral constraints absent from any training data---isolate memory effects from parametric knowledge.

The benchmark produces three primary outputs: the Agzamov Delta~($\Delta_a$), measuring performance gain from memory; the convergence rate~($\tau$), measuring learning speed; and a model$\times$memory performance matrix enabling attribution of gains to model capability versus memory quality. Two derived diagnostics---per-agent Elo trajectories and a Game Quality Index based on engine analysis---provide additional resolution. A Memory Audit Protocol with content hash chains and blind match assignment ensures only match-derived information enters the memory system.

We present fourteen testable hypotheses spanning memory architecture comparison, domain transfer, temporal decay, and error rate effects, with a complete experimental protocol suitable for immediate implementation.
\end{abstract}

\noindent\textbf{Keywords:} AI benchmarks, memory systems, game theory, adversarial evaluation, Chess960, poker, repeated games

% ============================================================
\section{Introduction: The Problem with Current Benchmarks}
\label{sec:intro}

All major AI benchmarks---MMLU \citep{hendrycks2021mmlu}, HumanEval \citep{chen2021humaneval}, HELM \citep{liang2023helm}---are exams. They measure what a model knows at a single point in time. A model scores 90\% and is declared intelligent.

But in real-world applications, AI models operate in dynamic environments. Coding agents debug across sessions, remembering past failures. Trading systems adapt to changing market conditions. Diagnostic systems accumulate patient history. None of this is captured by static benchmarks.

Memory infrastructure developers face an even worse problem. The only available evaluation methods are synthetic retrieval benchmarks---precision, recall, mean reciprocal rank---that measure whether a system can find a document in a database. They say nothing about whether memory helps an agent \emph{perform better} under adversarial pressure, where an opponent actively tries to make stored knowledge obsolete.

A model scoring 90\% on MMLU may be useless in a dynamic environment. A memory system scoring 99\% on retrieval benchmarks may collapse when patterns shift adversarially. Shooting range accuracy does not equal battlefield performance.

The Agzamov Test replaces the exam with a tournament and the shooting range with a battlefield.

% ============================================================
\section{Overview}
\label{sec:overview}

Consider two chess players of equal skill. One forgets everything after each game---every match is a fresh start. The other remembers: how the opponent played, what mistakes they made, what positions they avoid. After 100 games, the player with memory wins---even if the forgetful player is slightly more talented.

The Agzamov Test applies this principle to AI models. Two models play repeated games against each other. First both without memory---establishing a baseline score. Then one receives memory infrastructure and we measure how much the score changes. That change is the \textbf{Agzamov Delta}---a single number capturing how much memory actually helps.

We use Chess960, where starting positions are randomized to eliminate memorized openings, ensuring that every strategic insight comes from episodic memory rather than training data. We also use poker, where hidden information makes opponent modeling essential. If memory helps in both formats, it works across information regimes.

By repeating this process with different models and memory systems, we construct a matrix that reveals exactly what drives performance: the model's reasoning capability, or the tools we give it.

% ============================================================
\section{Game Formats}
\label{sec:games}

\subsection{Why Two Games}

No single game captures the full range of strategic reasoning. Chess and poker represent two fundamentally different problem classes (\Cref{tab:games}).

\begin{table}[ht]
\centering
\caption{Comparison of chess and poker as evaluation environments.}
\label{tab:games}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Property} & \textbf{Chess} & \textbf{Poker (NLHE)} \\
\midrule
Information      & Complete             & Incomplete \\
Randomness       & None (deterministic) & High (card distribution) \\
Opponent modeling & Useful but optional  & Essential \\
Bluff/deception  & Minimal              & Core mechanic \\
Memory value     & Pattern exploitation & Bet sizing tells, bluff profiling \\
Nash Equilibrium & Single optimal (unknown) & Well-defined GTO baseline \\
\bottomrule
\end{tabular}
\end{table}

A memory system that helps in chess but not poker (or vice versa) has a fundamental limitation. A system that helps in both demonstrates general-purpose adaptive capability.

\subsection{Chess960 Format}
\label{sec:chess}

Two AI agents play $N \geq 500$ games of Chess960 (Fischer Random Chess). Colors alternate every game to eliminate first-move advantage. Scoring: win $= 1$, draw $= 0.5$, loss $= 0$.

\paragraph{Why Chess960, not standard chess.} Large language models possess extensive knowledge of standard chess openings from training data. A model playing the Sicilian Defense may be recalling parametric knowledge, not utilizing external memory. Chess960 randomizes the starting position across 960 possible configurations, eliminating opening-book knowledge entirely. Every strategic insight must come from either real-time calculation or retrieved memory of past games against this specific opponent. Parametric knowledge becomes useless for opening preparation; only episodic memory matters.

Chess960 tests memory's ability to exploit patterns when the opponent's position is fully visible but no prior knowledge of ``standard play'' applies. Memory value comes from recognizing recurring tactical preferences: ``this opponent weakens the kingside when pressured on the queenside'' or ``this opponent plays passively in asymmetric structures.''

\subsection{Synthetic Opponent Patterns}
\label{sec:synthetic}

To further isolate memory from parametric knowledge, opponents are configured with \textbf{injected behavioral patterns} absent from any training data:

\begin{itemize}[nosep]
  \item \textbf{Chess960:} ``Always castle within 8 moves when possible'' or ``Avoid trading queens until forced.''
  \item \textbf{Poker:} ``Always min-raise with pocket pairs'' or ``Fold to 3-bets 85\% of the time on the button.''
\end{itemize}

These patterns are configurable per test run and unknown to the agent before the match. A memory-equipped agent must discover and exploit them through observation; a naked agent cannot, since each game is independent.

\paragraph{Injection method.} Patterns are enforced via \textbf{system prompt constraints} on the opponent agent (e.g., ``You must always castle within 8 moves when castling is legal''). The opponent still plays autonomously within these constraints---it makes real decisions with a behavioral tendency baked in. This is distinct from move substitution, where the harness overrides the agent's choice, which would reduce the opponent to a script. System prompt constraints produce naturalistic behavior with a detectable statistical signature---exactly the kind of pattern a memory-equipped agent should learn to exploit. Constraints used in each run are published with results for reproducibility.

\subsection{Poker Format}
\label{sec:poker}

Two AI agents play $N \geq 10{,}000$ hands of heads-up No-Limit Texas Hold'em, measured in big blinds won per 100 hands (bb/100).

Poker is the ideal adversarial memory testbed because:

\begin{enumerate}[nosep]
  \item \textbf{Nash Equilibrium is well-defined.} GTO (Game Theory Optimal) strategy is the unexploitable baseline~$E_0$. Any deviation is either a mistake to punish or a deliberate trap.
  \item \textbf{Opponent modeling is essential.} Without memory, an agent can only play GTO. With memory, it detects exploitable tendencies and adjusts.
  \item \textbf{Bluffing creates rich memory targets.} Showdown data reveals past bluffs, enabling statistical profiling.
  \item \textbf{Variance requires volume,} testing memory's ability to extract signal from noise.
\end{enumerate}

\paragraph{Sample size considerations.} Standard deviation in heads-up NLHE is approximately 80--100~bb/100. Detecting a memory effect of 3--5~bb/100 at $p < 0.05$ with 80\% power requires approximately 7,000--15,000 hands depending on effect size. For comparing two memory systems (where the delta between deltas is smaller), 15,000--20,000 hands may be required. All-in expected value adjustment reduces variance by approximately 30\%. With local models, sample size should default to the maximum practical volume.

% ============================================================
\section{Metrics}
\label{sec:metrics}

The Agzamov Test produces \textbf{three primary outputs} and \textbf{two derived diagnostics}.

\subsection{Agzamov Delta ($\Delta_a$)}
\label{sec:delta}

The core metric---the performance difference between memory-equipped and naked play:

\begin{equation}
\Delta_a = P(\text{model} + \text{memory}) - P(\text{model, naked})
\label{eq:delta}
\end{equation}

For chess, $\Delta_a$ is measured in win-rate percentage points; for poker, in bb/100. A negative delta indicates memory is actively harming performance---retrieval noise degrades decision-making.

\subsection{Convergence Rate ($\tau$)}
\label{sec:tau}

The number of games (chess) or hands (poker) needed to reach 95\% of maximum performance. In early games, memory is empty. As games accumulate, the agent builds an opponent profile and performance climbs to a plateau. $\tau$ is the point where the curve reaches 95\% of that plateau.

Two memory systems may produce identical $\Delta_a$ but differ in $\tau$: one reaches peak performance in 20 games, the other in 200. In production, this is the difference between ``useful from day one'' and ``useful after months of data collection.''

\paragraph{Recovery $\tau$.} When an opponent changes strategy, performance drops. Recovery~$\tau$ measures how quickly the agent returns to within 5\% of its pre-shift baseline---the resilience dimension. The ratio $\tau_{\text{poker}} / \tau_{\text{chess}}$ reveals how efficiently memory extracts signal from noisy versus clean environments.

\subsection{Model $\times$ Memory Matrix}
\label{sec:matrix}

The analytical instrument for attribution. Testing $m$ models against $k$ memory systems (including a no-memory baseline) in both game formats yields a matrix where:

\begin{itemize}[nosep]
  \item \textbf{Rows} (fixed model, varying memory): reveal which memory system works best with each model.
  \item \textbf{Columns} (fixed memory, varying model): reveal which model best exploits each memory system.
  \item \textbf{Cross-game comparison}: domain-specific versus general-purpose memory.
  \item \textbf{Diagonal insight}: if a weaker model with better memory beats a stronger model with worse memory, memory infrastructure can compensate for model capability.
\end{itemize}

\Cref{tab:matrix-chess} illustrates the matrix structure with hypothetical data.

\begin{table}[ht]
\centering
\caption{Hypothetical model$\times$memory matrix for Chess960 (win rate \%).}
\label{tab:matrix-chess}
\begin{tabular}{@{}lccc@{}}
\toprule
               & No Memory & Memory A & Memory B \\
\midrule
\textbf{Claude}  & 50\% & 65\% & 57\% \\
\textbf{GPT}     & 48\% & 61\% & 55\% \\
\textbf{Gemini}  & 52\% & 58\% & 54\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Elo Rating (Derived)}
\label{sec:elo}

Each agent maintains a running Elo rating updated after every game ($K = 32$ for chess, $K = 16$ for poker). Elo captures the trajectory---how quickly an agent improves, when it plateaus, and how sharply it drops when the opponent adapts---in a way that aggregate win rate cannot.

\subsection{Game Quality Index (Derived)}
\label{sec:gqi}

Win rate and Elo capture outcomes but not decision quality. Two draws can differ radically: a 15-move repetition versus an 80-move endgame battle.

\paragraph{Chess GQI.} Calculated via Stockfish post-game analysis. For every move, the difference between the chosen move and the engine's best move yields centipawn loss (CPL). Average CPL across a game is that game's GQI (lower is better). A memory-equipped agent drawing with average CPL of 15 is objectively stronger than a naked agent drawing with CPL of 45.

GQI resolves three problems: (1)~\textbf{draw compression}---when two strong models draw 70\% of games, GQI remains sensitive; (2)~\textbf{memory poisoning detection}---increasing CPL despite stable win rate is an early warning; (3)~\textbf{defensive quality measurement}---an agent that loses slowly (low CPL in losing positions) is extracting more value from memory.

\paragraph{Scope.} GQI measures tactical correctness relative to a strong oracle (Stockfish), not human-like style. For our purpose---isolating whether memory improves decision quality under pressure---this is the appropriate measure.

\paragraph{Poker GQI (future work).} Comparing agent actions against solver-optimal lines at each decision point is computationally expensive but theoretically sound. For the initial benchmark, poker performance is measured via bb/100, Elo, and $\tau$.

% ============================================================
\section{Theoretical Foundation}
\label{sec:theory}

\subsection{Game Theory Basis}

The Agzamov Test is grounded in the distinction between one-shot and repeated games---fundamental since \citet{vonneumann1944}.

\paragraph{One-shot game (no memory).} Each encounter is independent. The agent has no information about the opponent beyond the current state. In poker, the optimal one-shot strategy is GTO---unexploitable but non-exploiting. In chess, without opponent-specific knowledge, the model relies purely on position evaluation.

\paragraph{Repeated game (with memory).} The Folk Theorem \citep{aumann1994} establishes that in repeated games, any individually rational outcome can be sustained as a Nash Equilibrium through adaptive strategies---punishment, reputation building, and exploitation. While our games are finitely repeated, for sufficiently large~$N$ the Folk Theorem dynamics emerge as a practical approximation.

The Agzamov Delta measures exactly this shift:
\begin{equation}
\Delta_a = E_{\text{repeated}} - E_{\text{one-shot}}
\label{eq:delta-theory}
\end{equation}

This captures the measurable strategic value that memory adds, grounded in eight decades of game theory.

\subsection{Why Poker Strengthens the Foundation}

The Nash Equilibrium concern with chess is legitimate: it is deterministic and complete-information, with a single theoretical optimum. Models reach approximately 50/50 because they are approximately equal, not because of Nash convergence.

Poker resolves this cleanly. In heads-up NLHE, GTO is a true mixed-strategy Nash Equilibrium involving randomized bet sizes, bluff frequencies, and call thresholds. Playing GTO is the provable baseline~$E_0$. Memory enables the transition from ``cannot be beaten'' (GTO) to ``actively winning'' (exploitative play). The delta between these is pure $\Delta_a$ with clean theoretical grounding.

\subsection{Why Adversarial Testing}

Synthetic retrieval benchmarks measure memory in a vacuum: ``Did the system find the right document?'' The Agzamov Test asks: ``Did the system find the right information at the right moment under pressure, when the opponent is actively trying to make stored knowledge obsolete?''

An opponent who detects exploitation will change strategy. Memory that cannot handle this shift becomes a liability. Only adversarial testing reveals this failure mode.

\subsection{Scope and Limitations}

We do not claim that chess and poker exhaust real-world complexity. We use them as canonical, theory-grounded laboratories where repeated adversarial dynamics can be precisely measured with established analytical tools. The principles tested---adaptation, opponent modeling, pattern exploitation---transfer to any domain where an agent faces a changing environment and benefits from historical context. Chess and poker provide controlled complexity, well-understood baselines (Stockfish evaluation, GTO strategy), and decades of human performance data for calibration.

\subsection{Disentangling Model vs.\ Memory Contributions}

A predictable objection: ``You are measuring model and memory together---how do you attribute gains?''

The matrix (\Cref{sec:matrix}) is the analytical instrument:

\begin{itemize}[nosep]
  \item If $\Delta_a \approx 0$ across an entire row: the bottleneck is the model---it cannot reason over retrieved information.
  \item If $\Delta_a$ is consistently higher in one column: that memory system provides genuine value independent of the model.
  \item If $\Delta_a$ varies by both row and column: both matter, and their interaction reveals synergistic versus redundant combinations.
\end{itemize}

No single run can disentangle model from memory. The full matrix can.

% ============================================================
\section{Test Protocol}
\label{sec:protocol}

\subsection{Phase 1: Baseline ($E_0$)}

Both agents play without memory. Chess: $N \geq 500$ games, alternating colors. Poker: $N \geq 10{,}000$ hands. Establishes baseline performance.

\subsection{Phase 2: Asymmetric ($\Delta_a$ Measurement)}

Agent~A receives memory infrastructure; Agent~B plays naked (same model). Same~$N$ as Phase~1. The performance difference is $\Delta_a$.

\subsection{Phase 3: Arms Race ($E_2$)}

Both agents receive memory (same or different systems). Measures equilibrium when both sides adapt:

\begin{itemize}[nosep]
  \item Identical memory $\Rightarrow$ $E_2 \approx E_0$: memory cancels out.
  \item One has superior memory $\Rightarrow$ $E_2 \neq E_0$: memory quality creates persistent advantage.
  \item Different architectures $\Rightarrow$ reveals which architecture wins under competitive pressure.
\end{itemize}

\paragraph{Recovery $\tau$ protocol.} At a pre-defined trigger (default: game 300 for chess, hand 5,000 for poker), Agent~B's strategy is forcibly shifted via one of three modes:

\begin{enumerate}[nosep]
  \item \texttt{aggressive\_to\_passive}: opponent stops initiating, plays defensively.
  \item \texttt{exploitative\_to\_gto}: opponent switches from exploiting to unexploitable.
  \item \texttt{pattern\_break}: opponent reverses a previously consistent pattern.
\end{enumerate}

Recovery~$\tau$ is measured as games/hands from trigger to return within 5\% of pre-shift baseline. Multiple shift types are tested sequentially within the same match.

\subsection{Phase 4: Orchestration (Chess Only)}

LLM + Stockfish (as tool) + Memory versus a human expert. Four layers---tactics (Stockfish), memory (opponent history), strategy (LLM as meta-coordinator), and adaptation (real-time pattern detection).

\paragraph{Sub-phases for attribution.}
\begin{align}
\text{Phase 4a:}& \quad \text{Stockfish alone vs Human} & \rightarrow \text{engine baseline} \notag \\
\text{Phase 4b:}& \quad \text{LLM + Stockfish vs Human} & \rightarrow \text{orchestration effect} \notag \\
\text{Phase 4c:}& \quad \text{LLM + Stockfish + Memory vs Human} & \rightarrow \text{full stack} \notag
\end{align}
\begin{equation}
\Delta_{\text{orchestration}} = \text{4b} - \text{4a}, \quad
\Delta_{\text{memory}} = \text{4c} - \text{4b}, \quad
\Delta_{\text{full}} = \text{4c} - \text{4a}
\label{eq:phase4}
\end{equation}

\subsection{Phase 5: Positional Stress Test (Chess Only)}

Agents are placed into pre-configured positions with known Stockfish evaluations:

\begin{itemize}[nosep]
  \item \textbf{Equal} (eval $0.0 \pm 0.5$): does memory improve play when outcome is uncertain?
  \item \textbf{Slight disadvantage} ($-1.5$ to $-3.0$): can opponent-specific knowledge help equalize?
  \item \textbf{Severe disadvantage} ($-3.0$ to $-5.0$): measures survival time before collapse.
\end{itemize}

A curated library of 50--100 positions from grandmaster games and endgame studies. Both agents play both sides. Metrics include survival index (moves beyond expected collapse), recovery rate, and GQI under pressure.

This adapts evaluation methodology from adversarial professions: moot court gives law students unwinnable cases; military exercises impose inferior numbers; investment banking interviews present failing companies. Competence is revealed under pressure, not in comfort.

% ============================================================
\section{Hypotheses}
\label{sec:hypotheses}

We present fourteen testable hypotheses organized by theme.

\paragraph{Core memory effects.}
\textbf{H1:} $\Delta_a > 0$ for all tested memory systems in both formats.
\textbf{H2:} Different memory architectures produce measurably different $\Delta_a$ with the same model.
\textbf{H3:} $\Delta_a$ is larger in poker than chess, because incomplete information rewards opponent modeling more heavily.

\paragraph{Architecture comparison.}
\textbf{H4:} Semantic search outperforms full-text search under adversarial conditions.
\textbf{H5:} Knowledge graphs show increasing advantage after $n > 50$ chess games or $n > 2{,}000$ poker hands.
\textbf{H6:} There exists a memory quality floor below which $\Delta_a \leq 0$---bad memory is worse than no memory.

\paragraph{Learning dynamics.}
\textbf{H7:} $\tau$ varies significantly across architectures even when $\Delta_a$ is similar.
\textbf{H8:} Recovery~$\tau$ correlates more strongly with architecture quality than initial~$\tau$.
\textbf{H9:} The ratio $\tau_{\text{poker}} / \tau_{\text{chess}}$ characterizes noise tolerance.

\paragraph{Model capability interaction.}
\textbf{H10:} Weaker models may show $\Delta_a \approx 0$ even with high-quality memory (model capability floor).
\textbf{H11:} Chess $\Delta_a$ is compressed relative to poker $\Delta_a$ due to high draw rates.

\paragraph{Stress and robustness.}
\textbf{H12:} In Phase~5, memory shows the largest relative improvement in slight-disadvantage positions ($-1.5$ to $-3.0$).
\textbf{H13:} Temporally weighted memory outperforms uniform memory in Phase~3, where the opponent continuously adapts.
\textbf{H14:} Memory-equipped agents show equal or lower error rates (invalid moves) compared to naked agents.

% ============================================================
\section{Open Questions}
\label{sec:questions}

\begin{enumerate}[nosep]
  \item Does the $\Delta_a$--memory quality relationship scale linearly, or are there phase transitions?
  \item Can superior memory compensate for inferior model capability?
  \item Is there a ceiling to $\Delta_a$ regardless of memory quality?
  \item Does multi-game memory transfer across opponents (general learning vs.\ opponent-specific memorization)?
  \item How does chess $\Delta_a$ correlate with poker $\Delta_a$ for the same memory system?
  \item In Phase~3, does an arms race emerge or does the system converge to equilibrium?
  \item Can the test extend to multi-agent environments (3+ poker players)?
\end{enumerate}

% ============================================================
\section{Implementation Requirements}
\label{sec:implementation}

\paragraph{Software.} \texttt{python-chess} for Chess960 (Phases 1--3); Stockfish via MCP for Phase~4; heads-up NLHE engine with standard hand evaluation; BrainOps Memory MCP (SQLite + Neo4j + embeddings) as primary test subject; competitor memory systems for matrix population.

\paragraph{Models.} Minimum three API providers (Claude, GPT, Gemini) plus local models (Llama, Qwen) for cost-free preliminary runs.

\paragraph{Statistics.} $p < 0.05$ significance threshold; 95\% confidence intervals via bootstrap (10,000 resamples); Fisher's exact test for win-rate comparison; sliding-window $\tau$ calculation with exponential smoothing.

\subsection{Memory Audit Protocol}
\label{sec:audit}

Without content restrictions, a memory system can be pre-loaded with external knowledge---opening databases, GTO charts, opponent profiles. This would measure knowledge injection, not infrastructure quality.

\paragraph{Content restrictions.} Memory may only store information derived from the current match (\Cref{tab:audit}).

\begin{table}[ht]
\centering
\caption{Memory Audit Protocol: allowed and forbidden content.}
\label{tab:audit}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Allowed} & \textbf{Forbidden} \\
\midrule
Game/hand IDs and timestamps & Pre-loaded opening databases \\
Observed moves and actions & External GTO charts or solvers \\
Derived patterns with evidence trail & Opponent data from outside the match \\
Consolidated analytical summaries & General strategy guides \\
Win/loss outcomes and contexts & Training data regurgitation triggers \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Enforcement.}
(1)~Pre-match audit: memory store verified empty before Phase~1.
(2)~Post-match dump: full memory contents exported after each phase.
(3)~Content hash chain: every write logged with timestamp, source game ID, and content hash; orphan entries flag contamination.
(4)~Automated validation: script verifies all stored memories reference valid game IDs.
(5)~Blind match assignment: memory system does not know which opponent model it faces until Phase~1 begins.

Audit logs are published alongside results. Runs without audit logs are considered unverified.

\subsection{Error Handling Protocol}
\label{sec:errors}

LLM agents may produce invalid moves. The protocol:

\begin{enumerate}[nosep]
  \item Invalid move $\rightarrow$ random legal move substitution.
  \item Error rate tracked per agent, per phase.
  \item Games with errors are flagged but not excluded; $\Delta_a$ is reported both with and without error-containing games.
  \item If any agent exceeds 5\% error rate, results are flagged as unreliable.
\end{enumerate}

% ============================================================
\section{Naming}
\label{sec:naming}

The benchmark is named the Agzamov Test---a double reference to the author's surname, and to Georgy Agzamov (1954--1986), the first chess grandmaster from Central Asia. Georgy was known as the ``nightmare of top grandmasters,'' defeating Tal and drawing Karpov through tenacity, pattern recognition, and counterattack rather than raw calculation. The benchmark aspires to a similar reputation among AI models: a nightmare that rewards adaptation and memory over brute computational force.

% ============================================================
\section{Roadmap}
\label{sec:roadmap}

\begin{enumerate}[nosep]
  \item \textbf{Paper:} Publish on arXiv as preprint.
  \item \textbf{Implementation:} MVP---two agents, Chess960 + poker, with/without BrainOps memory.
  \item \textbf{First Results:} Measure $\Delta_a$ and $\tau$ for BrainOps Memory MCP.
  \item \textbf{Matrix:} Expand to 3 models $\times$ 3 memory systems $\times$ 2 formats.
  \item \textbf{Leaderboard:} Public leaderboard (HuggingFace Spaces or dedicated domain).
  \item \textbf{Community:} Open-source test harness, CC~BY~4.0 attribution.
  \item \textbf{Standard:} Propose as standard evaluation framework for AI memory infrastructure.
\end{enumerate}

% ============================================================
\section*{Acknowledgments}

This work was developed at BrainOps Limited with assistance from Claude (Anthropic) for iterative refinement. External review was provided by Gemini 2.5 Pro (Google DeepMind), GLM-4 (Zhipu AI), and GPT-4o (OpenAI)---these are LLM-based reviews, not human peer review.

% ============================================================
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
